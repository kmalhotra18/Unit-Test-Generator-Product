{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNK4reG/s2paZbWV7Ea+68r",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kmalhotra18/Unit-Test-Generator-Product/blob/main/Generate_Unit_Test_Cases_Product_Open_Source_Qwen.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Product that will **generate** **Unit Test cases**"
      ],
      "metadata": {
        "id": "hA8horsglQIE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uYmNwTHrlK_e"
      },
      "outputs": [],
      "source": [
        "# Step 1: Install necessary packages\n",
        "!pip install -q openai gradio transformers accelerate"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 2: Imports\n",
        "import gradio as gr\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from openai import OpenAI\n",
        "from google.colab import userdata\n",
        "import requests"
      ],
      "metadata": {
        "id": "9lhS6XG4lsW9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 3: Load API keys from Colab secrets\n",
        "OPENAI_API_KEY = userdata.get(\"OPENAI_API_KEY\")\n",
        "HUGGINGFACE_API_KEY = userdata.get(\"HF_TOKEN\")"
      ],
      "metadata": {
        "id": "DBGC3UOXlxvj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Qwen endpoint info\n",
        "CODE_QWEN_ID = \"Qwen/CodeQwen1.5-7B-Chat\"\n",
        "CODE_QWEN_URL = \"https://ebr1diyv7rshsjxu.us-east-1.aws.endpoints.huggingface.cloud\"  # Your Hugging Face Inference Endpoint"
      ],
      "metadata": {
        "id": "Yyw5fs74tPBV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 4: Initialize OpenAI client\n",
        "client = OpenAI(api_key=OPENAI_API_KEY)"
      ],
      "metadata": {
        "id": "2xkBn9qRqwho"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def load_qwen():\n",
        "#     global qwen_model, qwen_tokenizer\n",
        "#     if qwen_model is None or qwen_tokenizer is None:\n",
        "#         qwen_tokenizer = AutoTokenizer.from_pretrained(\n",
        "#             \"Qwen/CodeQwen1.5-7B-Chat\",\n",
        "#             token=HUGGINGFACE_API_KEY\n",
        "#         )\n",
        "#         qwen_model = AutoModelForCausalLM.from_pretrained(\n",
        "#             \"Qwen/CodeQwen1.5-7B-Chat\",\n",
        "#             torch_dtype=\"auto\",\n",
        "#             device_map=\"auto\",\n",
        "#             token=HUGGINGFACE_API_KEY\n",
        "#         )"
      ],
      "metadata": {
        "id": "q0xgcNtDq2lO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 6: Unified function to generate test cases - Unit test generation logic for both models\n",
        "def generate_unit_tests(code: str, model_choice: str) -> str:\n",
        "    prompt = f\"\"\"You are an expert Python developer. Based on the following code, generate complete and concise unit test cases using the unittest module in Python.\\n\\nCode:\\n{code}\\n\\nUnit Tests:\"\"\"\n",
        "\n",
        "    try:\n",
        "        if model_choice == \"GPT-4o (OpenAI)\":\n",
        "            response = client.chat.completions.create(\n",
        "                model=\"gpt-4o\",\n",
        "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "                temperature=0.3,\n",
        "                max_tokens=800,\n",
        "            )\n",
        "            return response.choices[0].message.content\n",
        "\n",
        "        elif model_choice == \"CodeQwen1.5-7B-Chat (HuggingFace Endpoint)\":\n",
        "            headers = {\n",
        "                \"Authorization\": f\"Bearer {HUGGINGFACE_API_KEY}\",\n",
        "                \"Content-Type\": \"application/json\"\n",
        "            }\n",
        "            payload = {\n",
        "                \"inputs\": [\n",
        "                    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "                    {\"role\": \"user\", \"content\": prompt}\n",
        "                ],\n",
        "                \"parameters\": {\n",
        "                    \"max_new_tokens\": 512,\n",
        "                    \"temperature\": 0.3\n",
        "                }\n",
        "            }\n",
        "\n",
        "            response = requests.post(CODE_QWEN_URL, headers=headers, json=payload)\n",
        "            if response.status_code == 200:\n",
        "                output = response.json()\n",
        "                return output[0][\"generated_text\"].split(prompt)[-1].strip()\n",
        "            else:\n",
        "                return f\"Hugging Face API error: {response.status_code} - {response.text}\"\n",
        "\n",
        "        else:\n",
        "            return \"Unsupported model selection.\"\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"Error generating unit tests:\\n\\n{str(e)}\""
      ],
      "metadata": {
        "id": "sjEjfLfDl0hr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Gradio interface\n",
        "\n",
        "def handle_input(code_input, file_input, model_choice):\n",
        "    if file_input is not None:\n",
        "        code = file_input.read().decode(\"utf-8\")\n",
        "    elif code_input.strip():\n",
        "        code = code_input\n",
        "    else:\n",
        "        return \"Please paste code or upload a Python file.\"\n",
        "    return generate_unit_tests(code, model_choice)\n",
        "\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"## ðŸ§ª Python Unit Test Generator\")\n",
        "    gr.Markdown(\"Choose a model, paste your code or upload a `.py` file, and click **Generate**.\")\n",
        "\n",
        "    model_dropdown = gr.Dropdown(\n",
        "        [\"GPT-4o (OpenAI)\", \"CodeQwen1.5-7B-Chat (HuggingFace Endpoint)\"],\n",
        "        label=\"Choose Model\",\n",
        "        value=\"GPT-4o (OpenAI)\"\n",
        "    )\n",
        "\n",
        "    with gr.Row():\n",
        "        code_input = gr.Textbox(lines=20, label=\"Paste Your Python Code\")\n",
        "        file_input = gr.File(label=\"Or Upload `.py` File\", file_types=[\".py\"])\n",
        "\n",
        "    generate_btn = gr.Button(\"Generate Unit Tests\")\n",
        "    output = gr.Textbox(label=\"Generated Unit Tests\", lines=20)\n",
        "\n",
        "    generate_btn.click(\n",
        "        fn=handle_input,\n",
        "        inputs=[code_input, file_input, model_dropdown],\n",
        "        outputs=output\n",
        "    )\n",
        "\n",
        "demo.launch()"
      ],
      "metadata": {
        "id": "QdZr6r8Ql5CZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "print(userdata.get(\"HF_TOKEN\"))\n"
      ],
      "metadata": {
        "id": "ZDJBbc-nue6g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from google.colab import userdata\n",
        "\n",
        "HUGGINGFACE_API_KEY = userdata.get(\"HF_TOKEN\")\n",
        "CODE_QWEN_URL = \"https://ebr1diyv7rshsjxu.us-east-1.aws.endpoints.huggingface.cloud\"\n",
        "\n",
        "headers = {\n",
        "    \"Authorization\": f\"Bearer {HUGGINGFACE_API_KEY}\",\n",
        "    \"Content-Type\": \"application/json\"\n",
        "}\n",
        "payload = {\n",
        "    \"inputs\": [\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "        {\"role\": \"user\", \"content\": \"Write a function to add two numbers in Python.\"}\n",
        "    ],\n",
        "    \"parameters\": {\n",
        "        \"max_new_tokens\": 100\n",
        "    }\n",
        "}\n",
        "\n",
        "response = requests.post(CODE_QWEN_URL, headers=headers, json=payload)\n",
        "print(\"Status:\", response.status_code)\n",
        "print(\"Response:\", response.text)\n"
      ],
      "metadata": {
        "id": "1MfQ6RCNvqbS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}